{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9032ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, n_heads):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(emb_dim, n_heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(emb_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 4 * emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * emb_dim, emb_dim)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        # 生成 causal mask，保证第 t 个位置只能看到 <= t 的位置\n",
    "        mask = torch.tril(torch.ones(T, T, device=x.device)).unsqueeze(0).repeat(B, 1, 1)\n",
    "        # nn.MultiheadAttention 需要 bool mask，True 表示被遮挡\n",
    "        attn_mask = ~mask.bool()[0]  # (T, T) bool，True 表示遮挡\n",
    "\n",
    "        attn_out, _ = self.attn(x, x, x, attn_mask=attn_mask, need_weights=False)\n",
    "        x = x + attn_out\n",
    "        x = self.norm1(x)\n",
    "        mlp_out = self.mlp(x)\n",
    "        x = x + mlp_out\n",
    "        return self.norm2(x)\n",
    "\n",
    "class TinyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=512, n_heads=16, n_layers=12, block_size=1024):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, block_size, emb_dim))\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerBlock(emb_dim, n_heads) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln = nn.LayerNorm(emb_dim)\n",
    "        self.fc = nn.Linear(emb_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tok_emb = self.token_embedding(x)\n",
    "        x = tok_emb + self.pos_embedding[:, :x.size(1), :]\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln(x)\n",
    "        logits = self.fc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62dae4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1618/1618 [10:27<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 5.6479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1618/1618 [10:27<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.9396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1618/1618 [10:27<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.6004\n"
     ]
    }
   ],
   "source": [
    "# fine_tune_alpaca.ipynb\n",
    "\n",
    "#  1. 导入库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from transformers import AutoTokenizer  # 可选，若你用 huggingface tokenizer\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "save_dir = \"finetune\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# 2. 加载你的 TinyTransformer 模型结构（假设你保存在 tiny_model.pt）\n",
    "# from your_model import TinyTransformer  # 替换为你的模型定义路径\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TinyTransformer(vocab_size=8192).to(device)\n",
    "# 加载模型 checkpoint\n",
    "checkpoint = torch.load(\"tiny_transformer_best.pth\", map_location=device)\n",
    "state_dict = checkpoint[\"model_state_dict\"]\n",
    "\n",
    "# 判断是否是 DataParallel 保存的模型（带 \"module.\" 前缀）\n",
    "if any(k.startswith(\"module.\") for k in state_dict.keys()):\n",
    "    # 去除 \"module.\" 前缀\n",
    "    state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "\n",
    "# 加载权重到模型\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "#  3. 冻结除最后两层 block 以外的所有参数\n",
    "for name, param in model.named_parameters():\n",
    "    if \"blocks.10\" in name or \"blocks.11\" in name or \"ln\" in name or \"fc\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# 4. 加载 tokenizer\n",
    "from tokenizers import Tokenizer\n",
    "tokenizer = Tokenizer.from_file(\"./data/tiny_tokenizer.json\")\n",
    "\n",
    "#  5. 加载 Alpaca 数据集\n",
    "alpaca_data = []\n",
    "with open(\"./data/alpaca-1k.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        alpaca_data.append(json.loads(line))\n",
    "\n",
    "class AlpacaDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, block_size=1024):\n",
    "        self.samples = []\n",
    "        self.tokenizer = tokenizer\n",
    "        for example in data:\n",
    "            prompt = f\"Instruction:\\n{example['instruction']}\\nInput:\\n{example['input']}\\nOutput:\\n{example['output']}\"\n",
    "            ids = tokenizer.encode(prompt).ids\n",
    "            assert isinstance(ids, list) and isinstance(ids[0], int), \"Tokenizer output format error\"\n",
    "            if len(ids) < block_size:\n",
    "                ids += [0] * (block_size - len(ids))  # padding\n",
    "            else:\n",
    "                ids = ids[:block_size]\n",
    "            self.samples.append(torch.tensor(ids))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.samples[idx][:-1]\n",
    "        y = self.samples[idx][1:]\n",
    "        return x, y\n",
    "\n",
    "dataset = AlpacaDataset(alpaca_data, tokenizer)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 6. 训练准备\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n",
    "pad_token_id = tokenizer.token_to_id(\"<pad>\")\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "\n",
    "# 7. 微调训练\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    total_loss = 0\n",
    "    for x, y in tqdm(loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        B, T, V = logits.shape\n",
    "        loss = loss_fn(logits.view(B*T, V), y.view(B*T))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} Loss: {total_loss / len(loader):.4f}\")\n",
    "\n",
    "# 8. 保存微调后的模型\n",
    "\n",
    "save_path=os.path.join(save_dir, \"tiny_model_finetuned_alpaca.pt\")\n",
    "torch.save(model.state_dict(),save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6c65b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, model, tokenizer, max_new_tokens=100, device=\"cuda\"):\n",
    "    from torch.nn import functional as F\n",
    "\n",
    "    input_ids = tokenizer.encode(prompt).ids\n",
    "    input_ids = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
    "\n",
    "    eos_token_id = tokenizer.token_to_id(\"<|endoftext|>\")  # 终止符\n",
    "    # print(eos_token_id)\n",
    "    model.eval()\n",
    "    for _ in range(max_new_tokens):\n",
    "        if input_ids.size(1) > model.pos_embedding.size(1):\n",
    "            input_ids = input_ids[:, -model.pos_embedding.size(1):]  # 截断上下文\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids)\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            # print(next_token.tolist)\n",
    "            # print(f\"Next token text: {tokenizer.decode([next_token.item()])}\")\n",
    "            # print(f\"Next token logits: {next_token_logits}\")\n",
    "            # print(f\"Top 5 probs: {torch.topk(probs, 5)}\")\n",
    "\n",
    "        # 拼接生成的新 token\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "\n",
    "        # 检查是否生成了 <eos>\n",
    "        if next_token.item() == eos_token_id:\n",
    "            print(\"here\")\n",
    "            break\n",
    "\n",
    "    output_ids = input_ids[0].tolist()\n",
    "    return tokenizer.decode(output_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ebcaeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bead/.conda/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343962757/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50 evaluation samples to 'model_eval_samples.csv'. You can now open and annotate it (accurate: y/n).\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import csv\n",
    "\n",
    "# 假设 alpaca_data 是完整的数据列表\n",
    "sample_50 = random.sample(alpaca_data, 50)\n",
    "\n",
    "# 用于保存最终写入 CSV 的数据\n",
    "csv_rows = []\n",
    "\n",
    "for i, sample in enumerate(sample_50):\n",
    "    prompt = f\"Instruction: {sample['instruction']}\\nInput: {sample['input']}\\nResponse:\"\n",
    "    response = generate_text(prompt, model, tokenizer)  # 你的生成函数\n",
    "    \n",
    "    csv_rows.append({\n",
    "        \"index\": i + 1,\n",
    "        \"instruction\": sample[\"instruction\"],\n",
    "        \"input\": sample[\"input\"],\n",
    "        \"reference_output\": sample[\"output\"],\n",
    "        \"model_response\": response,\n",
    "        \"accurate\": \"\"  # 留空，人工填写 y/n\n",
    "    })\n",
    "\n",
    "# 写入 CSV 文件\n",
    "csv_file = \"model_eval_samples.csv\"\n",
    "with open(csv_file, mode=\"w\", newline='', encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"index\", \"instruction\", \"input\", \"reference_output\", \"model_response\", \"accurate\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(csv_rows)\n",
    "\n",
    "print(f\"Saved 50 evaluation samples to '{csv_file}'. You can now open and annotate it (accurate: y/n).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "696f620e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurate responses: 0/50\n",
      "Accuracy: 0.00%\n",
      "Target not met (accuracy ≤ 60%)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "accurate_count = 0\n",
    "total = 0\n",
    "\n",
    "with open(\"model_eval_samples.csv\", mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        if row[\"accurate\"].strip().lower() == \"y\":\n",
    "            accurate_count += 1\n",
    "        total += 1\n",
    "\n",
    "accuracy = accurate_count / total * 100\n",
    "print(f\"Accurate responses: {accurate_count}/{total}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "if accuracy > 60:\n",
    "    print(\"Target met (accuracy > 60%)\")\n",
    "else:\n",
    "    print(\"Target not met (accuracy ≤ 60%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b59756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello, how are you? a customer interactions? an opportunity to store?comtes wear thank a number.\n",
      "Input:\n",
      "\n",
      "Output:\n",
      "H diseases and I made someone who is it? capable ben purchg is a tim of the text in burization and email. for maintaining a diet and work, making it can do not your needs to minimize outdoed date to carefully or simply of which only detail feedback behind time. Shideingenanceizekes, prepand and is have a\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Hello, how are you?\"\n",
    "response = generate_text(prompt, model, tokenizer)  \n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
