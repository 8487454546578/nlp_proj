{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9032ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, n_heads):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(emb_dim, n_heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(emb_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 4 * emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * emb_dim, emb_dim)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        # 生成 causal mask，保证第 t 个位置只能看到 <= t 的位置\n",
    "        mask = torch.tril(torch.ones(T, T, device=x.device)).unsqueeze(0).repeat(B, 1, 1)\n",
    "        # nn.MultiheadAttention 需要 bool mask，True 表示被遮挡\n",
    "        attn_mask = ~mask.bool()[0]  # (T, T) bool，True 表示遮挡\n",
    "\n",
    "        attn_out, _ = self.attn(x, x, x, attn_mask=attn_mask, need_weights=False)\n",
    "        x = x + attn_out\n",
    "        x = self.norm1(x)\n",
    "        mlp_out = self.mlp(x)\n",
    "        x = x + mlp_out\n",
    "        return self.norm2(x)\n",
    "\n",
    "class TinyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=512, n_heads=16, n_layers=12, block_size=512):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, block_size, emb_dim))\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerBlock(emb_dim, n_heads) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln = nn.LayerNorm(emb_dim)\n",
    "        self.fc = nn.Linear(emb_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tok_emb = self.token_embedding(x)\n",
    "        x = tok_emb + self.pos_embedding[:, :x.size(1), :]\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln(x)\n",
    "        logits = self.fc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dae4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine_tune_alpaca.ipynb\n",
    "\n",
    "# ✅ 1. 导入库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from transformers import AutoTokenizer  # 可选，若你用 huggingface tokenizer\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# ✅ 2. 加载你的 TinyTransformer 模型结构（假设你保存在 tiny_model.pt）\n",
    "# from your_model import TinyTransformer  # 替换为你的模型定义路径\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TinyTransformer(vocab_size=8192).to(device)\n",
    "# 加载检查点\n",
    "checkpoint = torch.load(\"tiny_transformer_checkpoint.pth\", map_location=device)\n",
    "\n",
    "# 加载模型权重\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "\n",
    "# ✅ 3. 冻结除最后两层 block 以外的所有参数\n",
    "for name, param in model.named_parameters():\n",
    "    if \"blocks.10\" in name or \"blocks.11\" in name or \"ln\" in name or \"fc\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# ✅ 4. 加载 tokenizer\n",
    "from tokenizers import Tokenizer\n",
    "tokenizer = Tokenizer.from_file(\"tiny_tokenizer.json\")\n",
    "\n",
    "# ✅ 5. 加载 Alpaca 数据集\n",
    "alpaca_data = []\n",
    "with open(\"alpaca-1k.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        alpaca_data.append(json.loads(line))\n",
    "\n",
    "class AlpacaDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, block_size=256):\n",
    "        self.samples = []\n",
    "        self.tokenizer = tokenizer\n",
    "        for example in data:\n",
    "            prompt = f\"Instruction:\\n{example['instruction']}\\nInput:\\n{example['input']}\\nOutput:\\n{example['output']}\"\n",
    "            ids = tokenizer.encode(prompt).ids\n",
    "            assert isinstance(ids, list) and isinstance(ids[0], int), \"Tokenizer output format error\"\n",
    "            if len(ids) < block_size:\n",
    "                ids += [0] * (block_size - len(ids))  # padding\n",
    "            else:\n",
    "                ids = ids[:block_size]\n",
    "            self.samples.append(torch.tensor(ids))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.samples[idx][:-1]\n",
    "        y = self.samples[idx][1:]\n",
    "        return x, y\n",
    "\n",
    "dataset = AlpacaDataset(alpaca_data, tokenizer)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# ✅ 6. 训练准备\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n",
    "pad_token_id = tokenizer.token_to_id(\"<pad>\")\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "\n",
    "# ✅ 7. 微调训练\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    total_loss = 0\n",
    "    for x, y in tqdm(loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        B, T, V = logits.shape\n",
    "        loss = loss_fn(logits.view(B*T, V), y.view(B*T))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} Loss: {total_loss / len(loader):.4f}\")\n",
    "\n",
    "# ✅ 8. 保存微调后的模型\n",
    "torch.save(model.state_dict(), \"tiny_model_finetuned_alpaca.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c65b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, model, tokenizer, max_new_tokens=100, device=\"cuda\"):\n",
    "    from torch.nn import functional as F\n",
    "\n",
    "    input_ids = tokenizer.encode(prompt).ids\n",
    "    input_ids = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
    "\n",
    "    eos_token_id = tokenizer.token_to_id(\"<|endoftext|>\")  # 终止符\n",
    "    # print(eos_token_id)\n",
    "    model.eval()\n",
    "    for _ in range(max_new_tokens):\n",
    "        if input_ids.size(1) > model.pos_embedding.size(1):\n",
    "            input_ids = input_ids[:, -model.pos_embedding.size(1):]  # 截断上下文\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids)\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            # print(next_token.tolist)\n",
    "            # print(f\"Next token text: {tokenizer.decode([next_token.item()])}\")\n",
    "            # print(f\"Next token logits: {next_token_logits}\")\n",
    "            # print(f\"Top 5 probs: {torch.topk(probs, 5)}\")\n",
    "\n",
    "        # 拼接生成的新 token\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "\n",
    "        # 检查是否生成了 <eos>\n",
    "        if next_token.item() == eos_token_id:\n",
    "            print(\"here\")\n",
    "            break\n",
    "\n",
    "    output_ids = input_ids[0].tolist()\n",
    "    return tokenizer.decode(output_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebcaeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# 假设 alpaca_data 是完整的数据列表\n",
    "sample_50 = random.sample(alpaca_data, 50)\n",
    "\n",
    "for i, sample in enumerate(sample_50):\n",
    "    prompt = f\"Instruction: {sample['instruction']}\\nInput: {sample['input']}\\nResponse:\"\n",
    "    response = generate_text(prompt, model, tokenizer)  # 你已有的生成函数\n",
    "    # print(f\"\\n=== Sample {i+1} ===\")\n",
    "    # print(f\"Instruction: {sample['instruction']}\")\n",
    "    # if sample[\"input\"]:\n",
    "    #     print(f\"Input: {sample['input']}\")\n",
    "    print(f\"Model Response: {response}\")\n",
    "    # print(f\"Reference Output: {sample['output']}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b59756",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Hello, how are you?\"\n",
    "response = generate_text(prompt, model, tokenizer)  # 你已有的生成函数\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
