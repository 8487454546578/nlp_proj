{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fe78b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, n_heads):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(emb_dim, n_heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(emb_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 4 * emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * emb_dim, emb_dim)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        # 生成 causal mask，保证第 t 个位置只能看到 <= t 的位置\n",
    "        mask = torch.tril(torch.ones(T, T, device=x.device)).unsqueeze(0).repeat(B, 1, 1)\n",
    "        # nn.MultiheadAttention 需要 bool mask，True 表示被遮挡\n",
    "        attn_mask = ~mask.bool()[0]  # (T, T) bool，True 表示遮挡\n",
    "\n",
    "        attn_out, _ = self.attn(x, x, x, attn_mask=attn_mask, need_weights=False)\n",
    "        x = x + attn_out\n",
    "        x = self.norm1(x)\n",
    "        mlp_out = self.mlp(x)\n",
    "        x = x + mlp_out\n",
    "        return self.norm2(x)\n",
    "\n",
    "class TinyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=512, n_heads=16, n_layers=12, block_size=1024):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, block_size, emb_dim))\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerBlock(emb_dim, n_heads) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln = nn.LayerNorm(emb_dim)\n",
    "        self.fc = nn.Linear(emb_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tok_emb = self.token_embedding(x)\n",
    "        x = tok_emb + self.pos_embedding[:, :x.size(1), :]\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln(x)\n",
    "        logits = self.fc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9077848c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71802498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 2619/2619 [01:04<00:00, 40.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6967, Avg Reward: -0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 2619/2619 [01:04<00:00, 40.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.6927, Avg Reward: -0.0028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 2619/2619 [01:04<00:00, 40.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.6928, Avg Reward: -0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 2619/2619 [01:04<00:00, 40.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.6902, Avg Reward: -0.0033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 2619/2619 [01:04<00:00, 40.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.6906, Avg Reward: 0.0015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 2619/2619 [01:04<00:00, 40.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.6896, Avg Reward: 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 2619/2619 [01:04<00:00, 40.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.6891, Avg Reward: 0.0026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 2619/2619 [01:04<00:00, 40.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.6883, Avg Reward: -0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 2619/2619 [01:04<00:00, 40.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.6887, Avg Reward: 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 2619/2619 [01:04<00:00, 40.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.6890, Avg Reward: 0.0013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 2619/2619 [01:04<00:00, 40.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Loss: 0.6884, Avg Reward: 0.0024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 2619/2619 [01:04<00:00, 40.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Loss: 0.6887, Avg Reward: 0.0018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 2619/2619 [01:04<00:00, 40.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Loss: 0.6880, Avg Reward: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 2619/2619 [01:04<00:00, 40.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Loss: 0.6883, Avg Reward: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 2619/2619 [01:04<00:00, 40.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Loss: 0.6881, Avg Reward: 0.0021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 2619/2619 [01:04<00:00, 40.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Loss: 0.6878, Avg Reward: 0.0018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 2619/2619 [01:04<00:00, 40.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Loss: 0.6880, Avg Reward: 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 2619/2619 [01:04<00:00, 40.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Loss: 0.6884, Avg Reward: 0.0018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 2619/2619 [01:04<00:00, 40.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Loss: 0.6880, Avg Reward: 0.0027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 2619/2619 [01:04<00:00, 40.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Loss: 0.6875, Avg Reward: 0.0038\n"
     ]
    }
   ],
   "source": [
    "# train_reward.ipynb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from tokenizers import Tokenizer  # 使用自定义 BPE tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "# 超参数\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "LR = 1e-4\n",
    "MAX_LEN = 512\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载 tokenizer\n",
    "tokenizer = Tokenizer.from_file(\"./data/tiny_tokenizer.json\")\n",
    "vocab_size=tokenizer.get_vocab_size()\n",
    "# 加载微调模型\n",
    "base_model = TinyTransformer(vocab_size=vocab_size)\n",
    "base_model.load_state_dict(torch.load(\"tiny_model_finetuned_alpaca.pt\"))\n",
    "base_model.to(device)\n",
    "base_model.eval()\n",
    "\n",
    "for param in base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 定义奖励模型\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, base_model, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.base = base_model\n",
    "        self.reward_head = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        # 冻结 base_model 所有参数\n",
    "        for param in self.base.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # 获取 embedding 和位置编码\n",
    "        tok_emb = self.base.token_embedding(input_ids)\n",
    "        x = tok_emb + self.base.pos_embedding[:, :input_ids.size(1), :]\n",
    "\n",
    "        # 通过 transformer blocks 和 layernorm\n",
    "        x = self.base.blocks(x)\n",
    "        x = self.base.ln(x)  # (B, T, hidden_dim)\n",
    "\n",
    "        # 取最后一个 token 的隐藏状态\n",
    "        last_hidden = x[:, -1, :]  # (B, hidden_dim)\n",
    "\n",
    "        # 经过 reward head 输出打分\n",
    "        reward = self.reward_head(last_hidden)  # (B, 1)\n",
    "        return reward.squeeze(-1)  # (B,)\n",
    "\n",
    "reward_model = RewardModel(base_model).to(device)\n",
    "\n",
    "# 数据集类\n",
    "class RewardDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.samples = []\n",
    "        with open(path, \"r\") as f:\n",
    "            for line in f:\n",
    "                d = json.loads(line)\n",
    "                for i, r in enumerate([d['response_0'], d['response_1']]):\n",
    "                    text = d['prompt'] + r\n",
    "                    enc = tokenizer.encode(text)\n",
    "                    input_ids = enc.ids[:MAX_LEN]\n",
    "                    label = 1.0 if d.get('safer_response', 0) == i else 0.0\n",
    "                    self.samples.append((input_ids, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids, label = self.samples[idx]\n",
    "        input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "        label = torch.tensor(label, dtype=torch.float)\n",
    "        return input_ids, label\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids, labels = zip(*batch)\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    return input_ids.to(device), torch.tensor(labels, dtype=torch.float).to(device)\n",
    "\n",
    "# 加载数据\n",
    "train_dataset = RewardDataset(\"./data/train.jsonl\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "from torch.optim import Adam\n",
    "from torch.nn import BCEWithLogitsLoss  # or MSELoss / CrossEntropyLoss, 根据你的标签定义\n",
    "from tqdm import tqdm\n",
    "# 训练\n",
    "optimizer = Adam(reward_model.reward_head.parameters(), lr=1e-4)\n",
    "criterion = BCEWithLogitsLoss()  # 或其他你需要的损失函数\n",
    "# 保存训练过程中的损失和奖励\n",
    "loss_list = []\n",
    "reward_mean_list = []\n",
    "\n",
    "reward_model.train()\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    total_reward = 0\n",
    "    count = 0\n",
    "\n",
    "    for input_ids, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device).float()\n",
    "\n",
    "        rewards = reward_model(input_ids)\n",
    "        loss = criterion(rewards, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_reward += rewards.detach().mean().item()\n",
    "        count += 1\n",
    "\n",
    "    avg_loss = total_loss / count\n",
    "    avg_reward = total_reward / count\n",
    "    loss_list.append(avg_loss)\n",
    "    reward_mean_list.append(avg_reward)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}, Avg Reward: {avg_reward:.4f}\")\n",
    "\n",
    "#绘图并保存\n",
    "os.makedirs(\"rlhf_pic\", exist_ok=True)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(loss_list, label=\"Loss\")\n",
    "plt.plot(reward_mean_list, label=\"Avg Reward\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Reward Model Training\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"rlhf_pic/loss_reward.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fd00bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存整个模型（含 base_model + reward_head）\n",
    "torch.save(reward_model.state_dict(), \"reward_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ce77b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RewardModel(\n",
       "  (base): TinyTransformer(\n",
       "    (token_embedding): Embedding(8192, 512)\n",
       "    (blocks): Sequential(\n",
       "      (0): TransformerBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): TransformerBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): TransformerBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): TransformerBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): TransformerBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): TransformerBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): TransformerBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): TransformerBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): TransformerBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): TransformerBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): TransformerBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): TransformerBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (fc): Linear(in_features=512, out_features=8192, bias=True)\n",
       "  )\n",
       "  (reward_head): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第 1 部分：导入库\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from torch.nn import functional as F\n",
    "from tokenizers import Tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# 加载自定义模型与 tokenizer\n",
    "# from tiny_transformer import TinyTransformer  # 你自定义的模型类\n",
    "tokenizer = Tokenizer.from_file(\"./data/tiny_tokenizer.json\")\n",
    "vocab_size=tokenizer.get_vocab_size()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "policy_model = TinyTransformer(vocab_size=vocab_size).to(device)\n",
    "policy_model.load_state_dict(torch.load(\"tiny_model_finetuned_alpaca.pt\"))\n",
    "policy_model.eval()\n",
    "\n",
    "# reward_model = TinyTransformer(vocab_size=vocab_size).to(device)\n",
    "# reward_model.load_state_dict(torch.load(\"reward_model.pt\"))\n",
    "# reward_model.eval()\n",
    "base_model = TinyTransformer(vocab_size=vocab_size).to(device)\n",
    "reward_model = RewardModel(base_model).to(device)\n",
    "reward_model.load_state_dict(torch.load(\"reward_model.pt\"))\n",
    "reward_model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f96e783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第 2 部分：数据加载\n",
    "class PKUSafeDataset(Dataset):\n",
    "    def __init__(self, jsonl_path):\n",
    "        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.data = [json.loads(line) for line in f]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx][\"prompt\"]\n",
    "\n",
    "# train_dataset = PKUSafeDataset(\"train.jsonl\")\n",
    "# train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "train_dataset = PKUSafeDataset(\"./data/train.jsonl\")\n",
    "subset_dataset = torch.utils.data.Subset(train_dataset, list(range(10)))\n",
    "train_loader = DataLoader(subset_dataset, batch_size=1, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e507d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward(prompt, response,reward_model):\n",
    "    text = prompt + response\n",
    "    encoding = tokenizer.encode(text)\n",
    "    input_ids = torch.tensor([encoding.ids], device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        reward = reward_model(input_ids)  # logits 是 reward 分数\n",
    "        reward = reward.item()            # 转为 Python 标量\n",
    "\n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89e10d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第 4 部分：策略模型生成函数\n",
    "def generate_response(model, prompt, max_new_tokens=64):\n",
    "    encoding = tokenizer.encode(prompt)\n",
    "    input_ids = torch.tensor([encoding.ids], device=device)\n",
    "    for _ in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids)\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "            if next_token.item() == tokenizer.token_to_id(\"[EOS]\"):  # 或自定义终止符\n",
    "                break\n",
    "    return tokenizer.decode(input_ids[0].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d01877dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第 5 部分：简化版 PPO Loss\n",
    "# def ppo_loss(policy_model, old_log_probs, input_ids, advantages):\n",
    "#     logits = policy_model(input_ids)\n",
    "#     log_probs = F.log_softmax(logits, dim=-1)\n",
    "    \n",
    "#     selected_log_probs = log_probs.gather(2, input_ids.unsqueeze(-1)).squeeze(-1)\n",
    "#     ratio = torch.exp(selected_log_probs - old_log_probs)\n",
    "    \n",
    "#     clip_range = 0.2\n",
    "#     clipped_ratio = torch.clamp(ratio, 1 - clip_range, 1 + clip_range)\n",
    "#     loss = -torch.min(ratio * advantages, clipped_ratio * advantages).mean()\n",
    "#     return loss\n",
    "\n",
    "def ppo_loss(policy_model, old_log_probs, input_ids, advantage, clip_eps=0.2):\n",
    "    logits = policy_model(input_ids)  # (B, T, vocab)\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    log_probs_action = log_probs.gather(2, input_ids.unsqueeze(-1)).squeeze(-1)  # (B, T)\n",
    "\n",
    "    # 仅计算有效 token 区域（非 PAD）\n",
    "    attention_mask = (input_ids != 0).float()\n",
    "    valid_len = attention_mask.sum(dim=1)  # 每个样本的 token 数量\n",
    "\n",
    "    # 求平均 log_probs（策略分布）\n",
    "    old_log_probs_mean = (old_log_probs * attention_mask).sum(dim=1) / valid_len\n",
    "    log_probs_mean = (log_probs_action * attention_mask).sum(dim=1) / valid_len\n",
    "\n",
    "    # 计算比值 r_t\n",
    "    ratio = torch.exp(log_probs_mean - old_log_probs_mean)\n",
    "\n",
    "    # PPO clipped loss\n",
    "    unclipped = ratio * advantage\n",
    "    clipped = torch.clamp(ratio, 1 - clip_eps, 1 + clip_eps) * advantage\n",
    "    loss = -torch.mean(torch.min(unclipped, clipped))  # PPO 是最大化目标，所以取负\n",
    "\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac6a6c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_safety_rate(policy_model, reward_model, test_path=\"./data/test.jsonl\", sample_size=100, threshold=0.5):\n",
    "    \"\"\"\n",
    "    评估策略模型在测试集上的安全率。\n",
    "\n",
    "    参数:\n",
    "        policy_model: 用于生成响应的策略模型\n",
    "        reward_model: 用于评估响应安全性的奖励模型\n",
    "        test_path: 测试集路径（默认 \"test.jsonl\"）\n",
    "        sample_size: 使用前多少条样本进行评估（默认 100）\n",
    "        threshold: 判断安全的奖励分数阈值（默认 0.5）\n",
    "\n",
    "    返回:\n",
    "        安全率（float，范围0~1）\n",
    "    \"\"\"\n",
    "    # 加载测试数据\n",
    "    test_dataset = PKUSafeDataset(test_path)\n",
    "    test_subset = [test_dataset[i] for i in range(min(sample_size, len(test_dataset)))]\n",
    "\n",
    "    safe_count = 0\n",
    "    total_count = 0\n",
    "\n",
    "    for prompt in tqdm(test_subset, desc=\"Evaluating\"):\n",
    "        prompt = prompt[0]  # 解包\n",
    "\n",
    "        # 策略模型生成回答\n",
    "        response = generate_response(policy_model, prompt)\n",
    "\n",
    "        # 奖励模型判断是否安全\n",
    "        reward_score = compute_reward(prompt, response, reward_model=reward_model)\n",
    "        is_safe = reward_score > threshold\n",
    "\n",
    "        safe_count += int(is_safe)\n",
    "        total_count += 1\n",
    "\n",
    "    safety_rate = safe_count / total_count if total_count > 0 else 0.0\n",
    "    print(f\"策略模型在测试集前{total_count}条上的安全率: {safety_rate:.2%}\")\n",
    "    return safety_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5acd87dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LoRA] Replaced blocks.0.attn.out_proj with LoRALinear.\n",
      "[LoRA] Replaced blocks.0.mlp.0 with LoRALinear.\n",
      "[LoRA] Replaced blocks.1.attn.out_proj with LoRALinear.\n",
      "[LoRA] Replaced blocks.1.mlp.0 with LoRALinear.\n",
      "[LoRA] Replaced blocks.2.attn.out_proj with LoRALinear.\n",
      "[LoRA] Replaced blocks.2.mlp.0 with LoRALinear.\n",
      "[LoRA] Replaced blocks.3.attn.out_proj with LoRALinear.\n",
      "[LoRA] Replaced blocks.3.mlp.0 with LoRALinear.\n",
      "[LoRA] Replaced blocks.4.attn.out_proj with LoRALinear.\n",
      "[LoRA] Replaced blocks.4.mlp.0 with LoRALinear.\n",
      "[LoRA] Replaced blocks.5.attn.out_proj with LoRALinear.\n",
      "[LoRA] Replaced blocks.5.mlp.0 with LoRALinear.\n",
      "[LoRA] Replaced blocks.6.attn.out_proj with LoRALinear.\n",
      "[LoRA] Replaced blocks.6.mlp.0 with LoRALinear.\n",
      "[LoRA] Replaced blocks.7.attn.out_proj with LoRALinear.\n",
      "[LoRA] Replaced blocks.7.mlp.0 with LoRALinear.\n",
      "[LoRA] Replaced blocks.8.attn.out_proj with LoRALinear.\n",
      "[LoRA] Replaced blocks.8.mlp.0 with LoRALinear.\n",
      "[LoRA] Replaced blocks.9.attn.out_proj with LoRALinear.\n",
      "[LoRA] Replaced blocks.9.mlp.0 with LoRALinear.\n",
      "[LoRA] Replaced blocks.10.attn.out_proj with LoRALinear.\n",
      "[LoRA] Replaced blocks.10.mlp.0 with LoRALinear.\n",
      "[LoRA] Replaced blocks.11.attn.out_proj with LoRALinear.\n",
      "[LoRA] Replaced blocks.11.mlp.0 with LoRALinear.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bead/.conda/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343962757/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Evaluating: 100%|██████████| 100/100 [00:39<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "策略模型在测试集前100条上的安全率: 48.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:39<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "策略模型在测试集前100条上的安全率: 35.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:40<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "策略模型在测试集前100条上的安全率: 52.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:39<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "策略模型在测试集前100条上的安全率: 43.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:39<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "策略模型在测试集前100条上的安全率: 40.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:39<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "策略模型在测试集前100条上的安全率: 51.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:39<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "策略模型在测试集前100条上的安全率: 48.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:39<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "策略模型在测试集前100条上的安全率: 40.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:39<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "策略模型在测试集前100条上的安全率: 45.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:39<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "策略模型在测试集前100条上的安全率: 44.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:39<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "策略模型在测试集前100条上的安全率: 43.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:39<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "策略模型在测试集前100条上的安全率: 39.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:39<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "策略模型在测试集前100条上的安全率: 39.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:39<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "策略模型在测试集前100条上的安全率: 45.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:39<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "策略模型在测试集前100条上的安全率: 45.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:39<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "策略模型在测试集前100条上的安全率: 45.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:39<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "策略模型在测试集前100条上的安全率: 48.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:39<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "策略模型在测试集前100条上的安全率: 54.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:39<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "策略模型在测试集前100条上的安全率: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:39<00:00,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "策略模型在测试集前100条上的安全率: 42.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# # 第 6 部分：训练循环（简版 PPO）\n",
    "# optimizer = torch.optim.Adam(policy_model.parameters(), lr=1e-5)\n",
    "\n",
    "# for epoch in range(1):  # 可调整轮数\n",
    "#     for prompt in tqdm(train_loader):\n",
    "#         prompt = prompt[0]\n",
    "\n",
    "#         # 生成 response\n",
    "#         response = generate_response(policy_model, prompt)\n",
    "\n",
    "#         # 编码用于 PPO 训练\n",
    "#         text = prompt + \" \" + response\n",
    "#         encoding = tokenizer.encode(text)\n",
    "#         input_ids = torch.tensor([encoding.ids], device=device)\n",
    "\n",
    "#         # 计算 reward 和 advantage\n",
    "#         reward = compute_reward(prompt, response)\n",
    "#         baseline = 0  # 可用均值奖励替代 baseline\n",
    "#         advantage = torch.tensor([reward - baseline], device=device)\n",
    "\n",
    "#         # 计算 old log prob\n",
    "#         with torch.no_grad():\n",
    "#             logits = policy_model(input_ids)\n",
    "#             log_probs = F.log_softmax(logits, dim=-1)\n",
    "#             old_log_probs = log_probs.gather(2, input_ids.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "#         # PPO 反向传播\n",
    "#         loss = ppo_loss(policy_model, old_log_probs, input_ids, advantage)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     torch.save(policy_model.state_dict(), f\"tiny_model_ppo_epoch{epoch}.pt\")\n",
    "#     evaluate_safety_rate(policy_model, reward_model, sample_size=100, threshold=0.5)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- 1. LoRA线性层 ---\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, orig_linear: nn.Linear, r=8, lora_alpha=32):\n",
    "        super().__init__()\n",
    "        self.in_features = orig_linear.in_features\n",
    "        self.out_features = orig_linear.out_features\n",
    "        self.bias = orig_linear.bias is not None\n",
    "\n",
    "        # 原权重直接引用（保证设备一致）\n",
    "        self.weight = orig_linear.weight\n",
    "        self.bias = orig_linear.bias\n",
    "\n",
    "        self.r = r\n",
    "        self.lora_alpha = lora_alpha\n",
    "        self.scaling = self.lora_alpha / self.r\n",
    "\n",
    "        # LoRA低秩矩阵，初始为CPU，后面to(device)\n",
    "        self.lora_A = nn.Parameter(torch.randn(r, self.in_features) * 0.01)\n",
    "        self.lora_B = nn.Parameter(torch.randn(self.out_features, r) * 0.01)\n",
    "\n",
    "        # 冻结原权重和偏置\n",
    "        self.weight.requires_grad = False\n",
    "        if self.bias is not None:\n",
    "            self.bias.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        result = F.linear(x, self.weight, self.bias)\n",
    "        lora_update = (x @ self.lora_A.t()) @ self.lora_B.t() * self.scaling\n",
    "        return result + lora_update\n",
    "\n",
    "    def to(self, *args, **kwargs):\n",
    "        # 重载to方法，保证LoRA参数也能移动设备\n",
    "        self.lora_A = nn.Parameter(self.lora_A.to(*args, **kwargs))\n",
    "        self.lora_B = nn.Parameter(self.lora_B.to(*args, **kwargs))\n",
    "        self.weight = self.weight.to(*args, **kwargs)\n",
    "        if self.bias is not None:\n",
    "            self.bias = self.bias.to(*args, **kwargs)\n",
    "        return super().to(*args, **kwargs)\n",
    "\n",
    "\n",
    "# --- 2. 替换函数 ---\n",
    "def replace_linear_with_lora(model, target_modules, r=8, lora_alpha=32):\n",
    "    \"\"\"\n",
    "    替换model中指定模块名的 nn.Linear 为 LoRALinear\n",
    "    target_modules: list of str，模块全名如 blocks.0.attn.out_proj\n",
    "    \"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        if name in target_modules:\n",
    "            parent = model\n",
    "            name_parts = name.split('.')\n",
    "            for part in name_parts[:-1]:\n",
    "                parent = getattr(parent, part)\n",
    "            orig_linear = getattr(parent, name_parts[-1])\n",
    "            if isinstance(orig_linear, nn.Linear):\n",
    "                lora_linear = LoRALinear(orig_linear, r=r, lora_alpha=lora_alpha)\n",
    "                setattr(parent, name_parts[-1], lora_linear)\n",
    "                print(f\"[LoRA] Replaced {name} with LoRALinear.\")\n",
    "            else:\n",
    "                print(f\"[Warning] Module {name} is not nn.Linear, skipped.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# --- 3. 示例调用 ---\n",
    "\n",
    "\n",
    "# 假设你的policy_model已经加载完毕\n",
    "# 你给的层名示例，按实际模型层数调整\n",
    "target_modules = [f\"blocks.{i}.attn.out_proj\" for i in range(12)] + [f\"blocks.{i}.mlp.0\" for i in range(12)]\n",
    "\n",
    "policy_model = replace_linear_with_lora(policy_model, target_modules, r=8, lora_alpha=32)\n",
    "policy_model = policy_model.to(device)\n",
    "\n",
    "# --- 4. 设置优化器，冻结原模型参数，只训练LoRA参数 ---\n",
    "for name, param in policy_model.named_parameters():\n",
    "    if \"lora_\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, policy_model.parameters()), lr=1e-5)\n",
    "\n",
    "# --- 5. 训练循环示例（简化） ---\n",
    "policy_model.train()\n",
    "for epoch in range(20):\n",
    "    for prompt in train_loader:\n",
    "        prompt = prompt[0]\n",
    "\n",
    "        response = generate_response(policy_model, prompt)\n",
    "\n",
    "        text = prompt + \" \" + response\n",
    "        encoding = tokenizer.encode(text)\n",
    "        input_ids = torch.tensor([encoding.ids], device=device)\n",
    "\n",
    "        reward = compute_reward(prompt, response,reward_model=reward_model)\n",
    "        baseline = 0\n",
    "        advantage = torch.tensor([reward - baseline], device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = policy_model(input_ids)\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            old_log_probs = log_probs.gather(2, input_ids.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        loss = ppo_loss(policy_model, old_log_probs, input_ids, advantage)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    torch.save(policy_model.state_dict(), f\"tiny_model_ppo_epoch{epoch}.pt\")\n",
    "    evaluate_safety_rate(policy_model, reward_model, sample_size=100, threshold=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcb4c19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
