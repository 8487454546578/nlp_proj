{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aacdcd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from tokenizers import Tokenizer, models, pre_tokenizers, trainers, decoders, processors\n",
    "\n",
    "# files = [\"train_sampled.txt\", \"TinyStoriesV2-GPT4-valid.txt\"]\n",
    "# tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
    "# tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "# trainer = trainers.BpeTrainer(vocab_size=8192, special_tokens=[\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\",\"<|endoftext|>\"])\n",
    "# tokenizer.train(files, trainer)\n",
    "\n",
    "# tokenizer.decoder = decoders.BPEDecoder()\n",
    "# tokenizer.post_processor = processors.TemplateProcessing(\n",
    "#     single=\"<bos> $A <eos>\",\n",
    "#     pair=\"<bos> $A <eos> $B:1 <eos>:1\",\n",
    "#     special_tokens=[(\"<bos>\", tokenizer.token_to_id(\"<bos>\")), (\"<eos>\", tokenizer.token_to_id(\"<eos>\"))],\n",
    "# )\n",
    "\n",
    "# tokenizer.save(\"tiny_tokenizer.json\")\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors\n",
    "\n",
    "tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
    "\n",
    "# ‰ΩøÁî® ByteLevel ÂàÜËØçÂô®ÂèØ‰ª•‰øùÁïôÁ©∫Ê†ºÁ≠âËæπÁïå‰ø°ÊÅØ\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=8192,\n",
    "    special_tokens=[\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\", \"<|endoftext|>\"]\n",
    ")\n",
    "\n",
    "tokenizer.train( [\"./data/train_sampled.txt\", \"./data/TinyStoriesV2-GPT4-valid.txt\", \"./data/alpaca_tokenizer_text.txt\"], trainer)\n",
    "\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"<bos> $A <eos>\",\n",
    "    pair=\"<bos> $A <eos> $B:1 <eos>:1\",\n",
    "    special_tokens=[\n",
    "        (\"<bos>\", tokenizer.token_to_id(\"<bos>\")),\n",
    "        (\"<eos>\", tokenizer.token_to_id(\"<eos>\"))\n",
    "    ],\n",
    ")\n",
    "\n",
    "tokenizer.save(\"./data/tiny_tokenizer.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22054cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "import torch\n",
    "# from torch.utils.data import Dataset\n",
    "\n",
    "# class TokenizedDataset(Dataset):\n",
    "#     def __init__(self, text, tokenizer, block_size):\n",
    "#         # text: str, tokenizer: Â∑≤Âä†ËΩΩÁöÑtokenizerÔºåblock_size: int\n",
    "#         self.token_ids = tokenizer.encode(text).ids\n",
    "#         self.block_size = block_size\n",
    "#         self.num_blocks = (len(self.token_ids) - 1) // block_size\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.num_blocks\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         start = idx * self.block_size\n",
    "#         end = start + self.block_size + 1\n",
    "#         chunk = self.token_ids[start:end]\n",
    "#         x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
    "#         y = torch.tensor(chunk[1:], dtype=torch.long)\n",
    "#         return x, y\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TokenizedDataset(Dataset):\n",
    "    def __init__(self, text, tokenizer, block_size=1024):\n",
    "        self.block_size = block_size\n",
    "        self.chunks = []\n",
    "\n",
    "        stories = text.split('<|endoftext|>')\n",
    "        for story in stories:\n",
    "            encoded = tokenizer.encode(story + '<|endoftext|>').ids\n",
    "            if len(encoded) < block_size + 1:\n",
    "                continue\n",
    "            for i in range(0, len(encoded) - block_size, block_size):\n",
    "                chunk = encoded[i:i + block_size + 1]\n",
    "                self.chunks.append(chunk)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.chunks[idx]\n",
    "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
    "        return x, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "126386c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, n_heads):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(emb_dim, n_heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(emb_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 4 * emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * emb_dim, emb_dim)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        # ÁîüÊàê causal maskÔºå‰øùËØÅÁ¨¨ t ‰∏™‰ΩçÁΩÆÂè™ËÉΩÁúãÂà∞ <= t ÁöÑ‰ΩçÁΩÆ\n",
    "        mask = torch.tril(torch.ones(T, T, device=x.device)).unsqueeze(0).repeat(B, 1, 1)\n",
    "        # nn.MultiheadAttention ÈúÄË¶Å bool maskÔºåTrue Ë°®Á§∫Ë¢´ÈÅÆÊå°\n",
    "        attn_mask = ~mask.bool()[0]  # (T, T) boolÔºåTrue Ë°®Á§∫ÈÅÆÊå°\n",
    "\n",
    "        attn_out, _ = self.attn(x, x, x, attn_mask=attn_mask, need_weights=False)\n",
    "        x = x + attn_out\n",
    "        x = self.norm1(x)\n",
    "        mlp_out = self.mlp(x)\n",
    "        x = x + mlp_out\n",
    "        return self.norm2(x)\n",
    "\n",
    "class TinyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=512, n_heads=16, n_layers=12, block_size=1024):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, block_size, emb_dim))\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerBlock(emb_dim, n_heads) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln = nn.LayerNorm(emb_dim)\n",
    "        self.fc = nn.Linear(emb_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tok_emb = self.token_embedding(x)\n",
    "        x = tok_emb + self.pos_embedding[:, :x.size(1), :]\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln(x)\n",
    "        logits = self.fc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b025753b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bead/.conda/envs/pytorch/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:32: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 4 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Âä†ËΩΩ tokenizer\n",
    "from tokenizers import Tokenizer as RawTokenizer\n",
    "raw_tokenizer = RawTokenizer.from_file(\"./data/tiny_tokenizer.json\")\n",
    "vocab_size = raw_tokenizer.get_vocab_size()\n",
    "\n",
    "# ËØªÊñáÊú¨\n",
    "with open(\"./data/train_sampled.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    train_text = f.read()\n",
    "with open(\"./data/TinyStoriesV2-GPT4-valid.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    valid_text = f.read()\n",
    "\n",
    "block_size = 1024\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = TokenizedDataset(train_text, raw_tokenizer, block_size)\n",
    "valid_dataset = TokenizedDataset(valid_text, raw_tokenizer, block_size)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size)\n",
    "\n",
    "model = TinyTransformer(vocab_size)\n",
    "if torch.cuda.device_count()>1:\n",
    "    model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "def get_loss(logits, targets):\n",
    "    B, T, V = logits.shape\n",
    "    logits = logits.view(B * T, V)\n",
    "    targets = targets.view(B * T)\n",
    "    return F.cross_entropy(logits, targets)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(val_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    for x, y in val_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = get_loss(logits, y)\n",
    "        total_loss += loss.item()\n",
    "        count += 1\n",
    "    model.train()\n",
    "    return math.exp(total_loss / count)  # ËÆ°ÁÆóPPL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfda76a4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d3c5652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# # torch.cuda.empty_cache()\n",
    "# save_path = \"tiny_transformer_checkpoint.pth\"\n",
    "\n",
    "# epochs = 1\n",
    "# for epoch in range(epochs):\n",
    "#     for x, y in train_loader:\n",
    "#         x, y = x.to(device), y.to(device)\n",
    "#         logits = model(x)\n",
    "#         loss = get_loss(logits, y)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     val_ppl = evaluate(valid_loader)\n",
    "#     print(f\"Epoch {epoch+1} | Val PPL: {val_ppl:.2f}\")\n",
    "\n",
    "#     torch.save({\n",
    "#         'model_state_dict': model.state_dict(),\n",
    "#         'optimizer_state_dict': optimizer.state_dict(),\n",
    "#         'epoch': epoch + 1,\n",
    "#         'val_ppl': val_ppl,\n",
    "#     }, save_path)\n",
    "#     print(f\"Ê®°ÂûãÂ∑≤‰øùÂ≠òÂà∞ {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f060188f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bead/.conda/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343962757/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 9.1772 | Val PPL: 2878.29\n",
      "Epoch 2 | Train Loss: 7.9645 | Val PPL: 1913.08\n",
      "Epoch 3 | Train Loss: 7.5823 | Val PPL: 1538.71\n",
      "Epoch 4 | Train Loss: 7.3777 | Val PPL: 1358.24\n",
      "Epoch 5 | Train Loss: 7.2337 | Val PPL: 1130.77\n",
      "Epoch 6 | Train Loss: 7.0428 | Val PPL: 1670.60\n",
      "Epoch 7 | Train Loss: 7.4055 | Val PPL: 918.97\n",
      "Epoch 8 | Train Loss: 6.8034 | Val PPL: 857.83\n",
      "Epoch 9 | Train Loss: 6.7225 | Val PPL: 773.41\n",
      "Epoch 10 | Train Loss: 6.6055 | Val PPL: 690.13\n",
      "Epoch 11 | Train Loss: 6.4791 | Val PPL: 612.50\n",
      "Epoch 12 | Train Loss: 6.3486 | Val PPL: 542.63\n",
      "Epoch 13 | Train Loss: 6.2185 | Val PPL: 493.86\n",
      "Epoch 14 | Train Loss: 6.1232 | Val PPL: 469.58\n",
      "Epoch 15 | Train Loss: 6.0433 | Val PPL: 441.05\n",
      "Epoch 16 | Train Loss: 5.9730 | Val PPL: 414.56\n",
      "Epoch 17 | Train Loss: 5.9076 | Val PPL: 399.88\n",
      "Epoch 18 | Train Loss: 5.8679 | Val PPL: 386.43\n",
      "Epoch 19 | Train Loss: 5.8271 | Val PPL: 371.70\n",
      "Epoch 20 | Train Loss: 5.7793 | Val PPL: 359.52\n",
      "Epoch 21 | Train Loss: 5.7364 | Val PPL: 352.53\n",
      "Epoch 22 | Train Loss: 5.7079 | Val PPL: 346.96\n",
      "Epoch 23 | Train Loss: 5.6851 | Val PPL: 339.75\n",
      "Epoch 24 | Train Loss: 5.6591 | Val PPL: 334.16\n",
      "Epoch 25 | Train Loss: 5.6381 | Val PPL: 330.59\n",
      "Epoch 26 | Train Loss: 5.6226 | Val PPL: 327.01\n",
      "Epoch 27 | Train Loss: 5.6049 | Val PPL: 323.11\n",
      "Epoch 28 | Train Loss: 5.5831 | Val PPL: 321.74\n",
      "Epoch 29 | Train Loss: 5.5678 | Val PPL: 320.45\n",
      "Epoch 30 | Train Loss: 5.5555 | Val PPL: 318.78\n",
      "Epoch 31 | Train Loss: 5.5464 | Val PPL: 317.34\n",
      "Epoch 32 | Train Loss: 5.5385 | Val PPL: 314.00\n",
      "Epoch 33 | Train Loss: 5.5212 | Val PPL: 312.83\n",
      "Epoch 34 | Train Loss: 5.5145 | Val PPL: 310.33\n",
      "Epoch 35 | Train Loss: 5.5041 | Val PPL: 305.60\n",
      "Epoch 36 | Train Loss: 5.4851 | Val PPL: 305.65\n",
      "Epoch 37 | Train Loss: 5.4782 | Val PPL: 298.16\n",
      "Epoch 38 | Train Loss: 5.4540 | Val PPL: 293.09\n",
      "Epoch 39 | Train Loss: 5.4374 | Val PPL: 288.61\n",
      "Epoch 40 | Train Loss: 5.4184 | Val PPL: 283.99\n",
      "Epoch 41 | Train Loss: 5.3998 | Val PPL: 275.60\n",
      "Epoch 42 | Train Loss: 5.3773 | Val PPL: 275.20\n",
      "Epoch 43 | Train Loss: 5.3568 | Val PPL: 264.22\n",
      "Epoch 44 | Train Loss: 5.3360 | Val PPL: 261.64\n",
      "Epoch 45 | Train Loss: 5.3022 | Val PPL: 250.39\n",
      "Epoch 46 | Train Loss: 5.2463 | Val PPL: 250.75\n",
      "Epoch 47 | Train Loss: 5.2233 | Val PPL: 347.10\n",
      "Epoch 48 | Train Loss: 5.6073 | Val PPL: 319.17\n",
      "Epoch 49 | Train Loss: 5.4945 | Val PPL: 329.10\n",
      "Epoch 50 | Train Loss: 5.5195 | Val PPL: 325.41\n",
      "Epoch 51 | Train Loss: 5.5021 | Val PPL: 315.16\n",
      "Epoch 52 | Train Loss: 5.4638 | Val PPL: 309.20\n",
      "Epoch 53 | Train Loss: 5.4129 | Val PPL: 289.51\n",
      "Epoch 54 | Train Loss: 5.3761 | Val PPL: 285.30\n",
      "Epoch 55 | Train Loss: 5.3542 | Val PPL: 287.99\n",
      "Epoch 56 | Train Loss: 5.3356 | Val PPL: 276.69\n",
      "Epoch 57 | Train Loss: 5.3156 | Val PPL: 275.45\n",
      "Epoch 58 | Train Loss: 5.2778 | Val PPL: 271.69\n",
      "Epoch 59 | Train Loss: 5.2608 | Val PPL: 275.68\n",
      "Epoch 60 | Train Loss: 5.3076 | Val PPL: 253.60\n",
      "Epoch 61 | Train Loss: 5.2145 | Val PPL: 17754.08\n",
      "Epoch 62 | Train Loss: 9.8232 | Val PPL: 299.28\n",
      "Epoch 63 | Train Loss: 5.4076 | Val PPL: 301.33\n",
      "Epoch 64 | Train Loss: 5.4188 | Val PPL: 294.23\n",
      "Epoch 65 | Train Loss: 5.3899 | Val PPL: 293.64\n",
      "Epoch 66 | Train Loss: 5.3763 | Val PPL: 299.02\n",
      "Epoch 67 | Train Loss: 5.3698 | Val PPL: 304.69\n",
      "Epoch 68 | Train Loss: 5.3648 | Val PPL: 298.55\n",
      "Epoch 69 | Train Loss: 5.3436 | Val PPL: 286.35\n",
      "Epoch 70 | Train Loss: 5.3179 | Val PPL: 280.90\n",
      "Epoch 71 | Train Loss: 5.3129 | Val PPL: 273.24\n",
      "Epoch 72 | Train Loss: 5.2930 | Val PPL: 267.09\n",
      "Epoch 73 | Train Loss: 5.2696 | Val PPL: 266.11\n",
      "Epoch 74 | Train Loss: 5.2568 | Val PPL: 264.42\n",
      "Epoch 75 | Train Loss: 5.2397 | Val PPL: 262.39\n",
      "Epoch 76 | Train Loss: 5.2225 | Val PPL: 261.57\n",
      "Epoch 77 | Train Loss: 5.2084 | Val PPL: 261.69\n",
      "Epoch 78 | Train Loss: 5.1995 | Val PPL: 260.37\n",
      "Epoch 79 | Train Loss: 5.1897 | Val PPL: 257.42\n",
      "Epoch 80 | Train Loss: 5.1738 | Val PPL: 255.02\n",
      "Epoch 81 | Train Loss: 5.1618 | Val PPL: 254.43\n",
      "Epoch 82 | Train Loss: 5.1540 | Val PPL: 253.75\n",
      "Epoch 83 | Train Loss: 5.1452 | Val PPL: 258.59\n",
      "Epoch 84 | Train Loss: 5.1648 | Val PPL: 261.04\n",
      "Epoch 85 | Train Loss: 5.1631 | Val PPL: 255.32\n",
      "Epoch 86 | Train Loss: 5.1319 | Val PPL: 254.62\n",
      "Epoch 87 | Train Loss: 5.1145 | Val PPL: 258.50\n",
      "Epoch 88 | Train Loss: 5.1203 | Val PPL: 252.77\n",
      "Epoch 89 | Train Loss: 5.1099 | Val PPL: 253.73\n",
      "Epoch 90 | Train Loss: 5.1149 | Val PPL: 249.57\n",
      "Epoch 91 | Train Loss: 5.0703 | Val PPL: 259.80\n",
      "Epoch 92 | Train Loss: 5.0869 | Val PPL: 248.85\n",
      "Epoch 93 | Train Loss: 5.0431 | Val PPL: 246.77\n",
      "Epoch 94 | Train Loss: 5.0459 | Val PPL: 245.26\n",
      "Epoch 95 | Train Loss: 5.0346 | Val PPL: 244.39\n",
      "Epoch 96 | Train Loss: 5.0102 | Val PPL: 247.09\n",
      "Epoch 97 | Train Loss: 5.0076 | Val PPL: 240.84\n",
      "Epoch 98 | Train Loss: 4.9869 | Val PPL: 235.44\n",
      "Epoch 99 | Train Loss: 4.9757 | Val PPL: 233.06\n",
      "Epoch 100 | Train Loss: 4.9675 | Val PPL: 230.63\n",
      "Epoch 101 | Train Loss: 4.9475 | Val PPL: 230.66\n",
      "Epoch 102 | Train Loss: 4.9378 | Val PPL: 228.44\n",
      "Epoch 103 | Train Loss: 4.9241 | Val PPL: 223.77\n",
      "Epoch 104 | Train Loss: 4.9108 | Val PPL: 220.55\n",
      "Epoch 105 | Train Loss: 4.8988 | Val PPL: 219.09\n",
      "Epoch 106 | Train Loss: 4.8831 | Val PPL: 218.30\n",
      "Epoch 107 | Train Loss: 4.8730 | Val PPL: 214.47\n",
      "Epoch 108 | Train Loss: 4.8580 | Val PPL: 211.96\n",
      "Epoch 109 | Train Loss: 4.8453 | Val PPL: 210.82\n",
      "Epoch 110 | Train Loss: 4.8317 | Val PPL: 209.38\n",
      "Epoch 111 | Train Loss: 4.8209 | Val PPL: 207.04\n",
      "Epoch 112 | Train Loss: 4.8052 | Val PPL: 204.34\n",
      "Epoch 113 | Train Loss: 4.7936 | Val PPL: 202.51\n",
      "Epoch 114 | Train Loss: 4.7791 | Val PPL: 202.63\n",
      "Epoch 115 | Train Loss: 4.7690 | Val PPL: 199.65\n",
      "Epoch 116 | Train Loss: 4.7554 | Val PPL: 199.68\n",
      "Epoch 117 | Train Loss: 4.7462 | Val PPL: 197.29\n",
      "Epoch 118 | Train Loss: 4.7367 | Val PPL: 200.73\n",
      "Epoch 119 | Train Loss: 4.7350 | Val PPL: 200.05\n",
      "Epoch 120 | Train Loss: 4.7577 | Val PPL: 197.73\n",
      "Epoch 121 | Train Loss: 4.7062 | Val PPL: 194.47\n",
      "Epoch 122 | Train Loss: 4.6775 | Val PPL: 193.68\n",
      "Epoch 123 | Train Loss: 4.6798 | Val PPL: 193.00\n",
      "Epoch 124 | Train Loss: 4.6519 | Val PPL: 193.00\n",
      "Epoch 125 | Train Loss: 4.6376 | Val PPL: 189.64\n",
      "Epoch 126 | Train Loss: 4.6324 | Val PPL: 187.37\n",
      "Epoch 127 | Train Loss: 4.6034 | Val PPL: 189.11\n",
      "Epoch 128 | Train Loss: 4.5932 | Val PPL: 183.40\n",
      "Epoch 129 | Train Loss: 4.5703 | Val PPL: 182.61\n",
      "Epoch 130 | Train Loss: 4.5644 | Val PPL: 186.00\n",
      "Epoch 131 | Train Loss: 4.5717 | Val PPL: 183.78\n",
      "Epoch 132 | Train Loss: 4.5387 | Val PPL: 181.15\n",
      "Epoch 133 | Train Loss: 4.5560 | Val PPL: 174.27\n",
      "Epoch 134 | Train Loss: 4.4920 | Val PPL: 176.25\n",
      "Epoch 135 | Train Loss: 4.4843 | Val PPL: 169.77\n",
      "Epoch 136 | Train Loss: 4.4500 | Val PPL: 164.79\n",
      "Epoch 137 | Train Loss: 4.4345 | Val PPL: 163.64\n",
      "Epoch 138 | Train Loss: 4.3909 | Val PPL: 168.31\n",
      "Epoch 139 | Train Loss: 4.3884 | Val PPL: 160.08\n",
      "Epoch 140 | Train Loss: 4.3433 | Val PPL: 158.88\n",
      "Epoch 141 | Train Loss: 4.3305 | Val PPL: 158.18\n",
      "Epoch 142 | Train Loss: 4.2984 | Val PPL: 159.20\n",
      "Epoch 143 | Train Loss: 4.2725 | Val PPL: 151.51\n",
      "Epoch 144 | Train Loss: 4.2451 | Val PPL: 150.10\n",
      "Epoch 145 | Train Loss: 4.2257 | Val PPL: 152.08\n",
      "Epoch 146 | Train Loss: 4.2527 | Val PPL: 158.06\n",
      "Epoch 147 | Train Loss: 4.2219 | Val PPL: 151.86\n",
      "Epoch 148 | Train Loss: 4.2820 | Val PPL: 151.91\n",
      "Epoch 149 | Train Loss: 4.2547 | Val PPL: 148.27\n",
      "Epoch 150 | Train Loss: 4.1639 | Val PPL: 150.05\n",
      "Epoch 151 | Train Loss: 4.1504 | Val PPL: 141.48\n",
      "Epoch 152 | Train Loss: 4.0886 | Val PPL: 140.52\n",
      "Epoch 153 | Train Loss: 4.0890 | Val PPL: 137.43\n",
      "Epoch 154 | Train Loss: 4.0695 | Val PPL: 135.24\n",
      "Epoch 155 | Train Loss: 4.0020 | Val PPL: 137.37\n",
      "Epoch 156 | Train Loss: 3.9758 | Val PPL: 138.63\n",
      "Epoch 157 | Train Loss: 3.9659 | Val PPL: 133.84\n",
      "Epoch 158 | Train Loss: 3.9276 | Val PPL: 132.67\n",
      "Epoch 159 | Train Loss: 3.8905 | Val PPL: 131.02\n",
      "Epoch 160 | Train Loss: 3.8703 | Val PPL: 128.81\n",
      "Epoch 161 | Train Loss: 3.8403 | Val PPL: 127.68\n",
      "Epoch 162 | Train Loss: 3.8081 | Val PPL: 126.67\n",
      "Epoch 163 | Train Loss: 3.7774 | Val PPL: 127.58\n",
      "Epoch 164 | Train Loss: 3.7534 | Val PPL: 127.47\n",
      "Epoch 165 | Train Loss: 3.7185 | Val PPL: 127.50\n",
      "Epoch 166 | Train Loss: 3.6921 | Val PPL: 126.86\n",
      "Epoch 167 | Train Loss: 3.6674 | Val PPL: 126.22\n",
      "Epoch 168 | Train Loss: 3.6332 | Val PPL: 125.92\n",
      "Epoch 169 | Train Loss: 3.6061 | Val PPL: 124.46\n",
      "Epoch 170 | Train Loss: 3.5766 | Val PPL: 124.77\n",
      "Epoch 171 | Train Loss: 3.5474 | Val PPL: 123.47\n",
      "Epoch 172 | Train Loss: 3.5157 | Val PPL: 122.79\n",
      "Epoch 173 | Train Loss: 3.4875 | Val PPL: 124.37\n",
      "Epoch 174 | Train Loss: 3.4581 | Val PPL: 122.61\n",
      "Epoch 175 | Train Loss: 3.4261 | Val PPL: 123.79\n",
      "Epoch 176 | Train Loss: 3.4005 | Val PPL: 121.36\n",
      "Epoch 177 | Train Loss: 3.3814 | Val PPL: 132.16\n",
      "Epoch 178 | Train Loss: 3.4084 | Val PPL: 130.89\n",
      "Epoch 179 | Train Loss: 3.6090 | Val PPL: 131.92\n",
      "Epoch 180 | Train Loss: 3.3772 | Val PPL: 131.78\n",
      "Epoch 181 | Train Loss: 3.3943 | Val PPL: 126.59\n",
      "Epoch 182 | Train Loss: 3.2988 | Val PPL: 123.51\n",
      "Epoch 183 | Train Loss: 3.2966 | Val PPL: 124.69\n",
      "Epoch 184 | Train Loss: 3.2486 | Val PPL: 124.92\n",
      "Epoch 185 | Train Loss: 3.2057 | Val PPL: 130.72\n",
      "Epoch 186 | Train Loss: 3.1908 | Val PPL: 123.82\n",
      "Epoch 187 | Train Loss: 3.1546 | Val PPL: 129.93\n",
      "Epoch 188 | Train Loss: 3.1473 | Val PPL: 123.09\n",
      "Epoch 189 | Train Loss: 3.1445 | Val PPL: 134.16\n",
      "Epoch 190 | Train Loss: 3.0831 | Val PPL: 126.54\n",
      "Epoch 191 | Train Loss: 3.0576 | Val PPL: 128.30\n",
      "Epoch 192 | Train Loss: 3.0180 | Val PPL: 129.40\n",
      "Epoch 193 | Train Loss: 2.9786 | Val PPL: 128.15\n",
      "Epoch 194 | Train Loss: 2.9561 | Val PPL: 129.14\n",
      "Epoch 195 | Train Loss: 2.9074 | Val PPL: 132.72\n",
      "Epoch 196 | Train Loss: 2.8948 | Val PPL: 130.33\n",
      "Epoch 197 | Train Loss: 2.8553 | Val PPL: 130.33\n",
      "Epoch 198 | Train Loss: 2.8230 | Val PPL: 132.24\n",
      "Epoch 199 | Train Loss: 2.8049 | Val PPL: 131.25\n",
      "Epoch 200 | Train Loss: 2.7635 | Val PPL: 134.14\n",
      "Epoch 201 | Train Loss: 2.7443 | Val PPL: 139.07\n",
      "Epoch 202 | Train Loss: 2.7258 | Val PPL: 139.40\n",
      "Epoch 203 | Train Loss: 2.7176 | Val PPL: 139.51\n",
      "Epoch 204 | Train Loss: 2.7086 | Val PPL: 136.81\n",
      "Epoch 205 | Train Loss: 2.6705 | Val PPL: 136.33\n",
      "Epoch 206 | Train Loss: 2.6002 | Val PPL: 136.53\n",
      "Epoch 207 | Train Loss: 2.5962 | Val PPL: 140.55\n",
      "Epoch 208 | Train Loss: 2.5574 | Val PPL: 141.58\n",
      "Epoch 209 | Train Loss: 2.5085 | Val PPL: 142.43\n",
      "Epoch 210 | Train Loss: 2.5067 | Val PPL: 144.53\n",
      "Epoch 211 | Train Loss: 2.4489 | Val PPL: 146.78\n",
      "Epoch 212 | Train Loss: 2.4320 | Val PPL: 146.93\n",
      "Epoch 213 | Train Loss: 2.3952 | Val PPL: 151.40\n",
      "Epoch 214 | Train Loss: 2.3685 | Val PPL: 152.71\n",
      "Epoch 215 | Train Loss: 2.3389 | Val PPL: 150.79\n",
      "Epoch 216 | Train Loss: 2.3088 | Val PPL: 157.68\n",
      "Epoch 217 | Train Loss: 2.2842 | Val PPL: 155.41\n",
      "Epoch 218 | Train Loss: 2.2497 | Val PPL: 160.40\n",
      "Epoch 219 | Train Loss: 2.2320 | Val PPL: 166.85\n",
      "Epoch 220 | Train Loss: 2.2092 | Val PPL: 166.32\n",
      "Epoch 221 | Train Loss: 2.1842 | Val PPL: 164.71\n",
      "Epoch 222 | Train Loss: 2.1771 | Val PPL: 185.50\n",
      "Epoch 223 | Train Loss: 2.1779 | Val PPL: 168.26\n",
      "Epoch 224 | Train Loss: 2.1327 | Val PPL: 172.35\n",
      "Epoch 225 | Train Loss: 2.0763 | Val PPL: 182.08\n",
      "Epoch 226 | Train Loss: 2.0775 | Val PPL: 172.22\n",
      "Epoch 227 | Train Loss: 2.0273 | Val PPL: 179.55\n",
      "Epoch 228 | Train Loss: 2.0075 | Val PPL: 186.06\n",
      "Epoch 229 | Train Loss: 1.9758 | Val PPL: 194.62\n",
      "Epoch 230 | Train Loss: 1.9756 | Val PPL: 180.75\n",
      "Epoch 231 | Train Loss: 1.9414 | Val PPL: 196.02\n",
      "Epoch 232 | Train Loss: 1.9154 | Val PPL: 198.13\n",
      "Epoch 233 | Train Loss: 1.8710 | Val PPL: 194.35\n",
      "Epoch 234 | Train Loss: 1.8412 | Val PPL: 197.16\n",
      "Epoch 235 | Train Loss: 1.8142 | Val PPL: 208.08\n",
      "Epoch 236 | Train Loss: 1.7976 | Val PPL: 204.34\n",
      "Epoch 237 | Train Loss: 1.7689 | Val PPL: 207.94\n",
      "Epoch 238 | Train Loss: 1.7415 | Val PPL: 208.56\n",
      "Epoch 239 | Train Loss: 1.7234 | Val PPL: 217.14\n",
      "Epoch 240 | Train Loss: 1.6969 | Val PPL: 219.38\n",
      "Epoch 241 | Train Loss: 1.6710 | Val PPL: 219.63\n",
      "Epoch 242 | Train Loss: 1.6496 | Val PPL: 233.86\n",
      "Epoch 243 | Train Loss: 1.6275 | Val PPL: 228.71\n",
      "Epoch 244 | Train Loss: 1.6121 | Val PPL: 239.32\n",
      "Epoch 245 | Train Loss: 1.6219 | Val PPL: 242.80\n",
      "Epoch 246 | Train Loss: 1.5812 | Val PPL: 232.69\n",
      "Epoch 247 | Train Loss: 1.5503 | Val PPL: 254.58\n",
      "Epoch 248 | Train Loss: 1.5372 | Val PPL: 250.88\n",
      "Epoch 249 | Train Loss: 1.5109 | Val PPL: 233.93\n",
      "Epoch 250 | Train Loss: 1.5230 | Val PPL: 269.44\n",
      "Epoch 251 | Train Loss: 1.5151 | Val PPL: 255.90\n",
      "Epoch 252 | Train Loss: 1.5294 | Val PPL: 254.10\n",
      "Epoch 253 | Train Loss: 1.5142 | Val PPL: 258.03\n",
      "Epoch 254 | Train Loss: 1.4365 | Val PPL: 278.89\n",
      "Epoch 255 | Train Loss: 1.4609 | Val PPL: 267.92\n",
      "Epoch 256 | Train Loss: 1.4390 | Val PPL: 269.74\n",
      "Epoch 257 | Train Loss: 1.4419 | Val PPL: 275.43\n",
      "Epoch 258 | Train Loss: 1.3636 | Val PPL: 271.67\n",
      "Epoch 259 | Train Loss: 1.3591 | Val PPL: 278.16\n",
      "Epoch 260 | Train Loss: 1.3236 | Val PPL: 285.83\n",
      "Epoch 261 | Train Loss: 1.3006 | Val PPL: 280.28\n",
      "Epoch 262 | Train Loss: 1.2686 | Val PPL: 291.69\n",
      "Epoch 263 | Train Loss: 1.2364 | Val PPL: 304.34\n",
      "Epoch 264 | Train Loss: 1.2267 | Val PPL: 296.52\n",
      "Epoch 265 | Train Loss: 1.2024 | Val PPL: 320.23\n",
      "Epoch 266 | Train Loss: 1.1757 | Val PPL: 315.57\n",
      "Epoch 267 | Train Loss: 1.1580 | Val PPL: 312.04\n",
      "Epoch 268 | Train Loss: 1.1286 | Val PPL: 326.57\n",
      "Epoch 269 | Train Loss: 1.1094 | Val PPL: 327.26\n",
      "Epoch 270 | Train Loss: 1.0884 | Val PPL: 344.55\n",
      "Epoch 271 | Train Loss: 1.0707 | Val PPL: 359.82\n",
      "Epoch 272 | Train Loss: 1.0417 | Val PPL: 353.87\n",
      "Epoch 273 | Train Loss: 1.0310 | Val PPL: 346.17\n",
      "Epoch 274 | Train Loss: 1.0098 | Val PPL: 359.97\n",
      "Epoch 275 | Train Loss: 0.9872 | Val PPL: 378.74\n",
      "Epoch 276 | Train Loss: 0.9688 | Val PPL: 377.78\n",
      "Epoch 277 | Train Loss: 0.9522 | Val PPL: 385.80\n",
      "Epoch 278 | Train Loss: 0.9329 | Val PPL: 394.39\n",
      "Epoch 279 | Train Loss: 0.9130 | Val PPL: 399.73\n",
      "Epoch 280 | Train Loss: 0.8995 | Val PPL: 410.65\n",
      "Epoch 281 | Train Loss: 0.8789 | Val PPL: 411.05\n",
      "Epoch 282 | Train Loss: 0.8635 | Val PPL: 417.79\n",
      "Epoch 283 | Train Loss: 0.8448 | Val PPL: 443.06\n",
      "Epoch 284 | Train Loss: 0.8327 | Val PPL: 443.24\n",
      "Epoch 285 | Train Loss: 0.8138 | Val PPL: 449.52\n",
      "Epoch 286 | Train Loss: 0.7988 | Val PPL: 461.93\n",
      "Epoch 287 | Train Loss: 0.7846 | Val PPL: 466.23\n",
      "Epoch 288 | Train Loss: 0.7728 | Val PPL: 477.92\n",
      "Epoch 289 | Train Loss: 0.7614 | Val PPL: 482.09\n",
      "Epoch 290 | Train Loss: 0.7586 | Val PPL: 506.34\n",
      "Epoch 291 | Train Loss: 0.7912 | Val PPL: 546.81\n",
      "Epoch 292 | Train Loss: 0.8581 | Val PPL: 487.35\n",
      "Epoch 293 | Train Loss: 0.8311 | Val PPL: 579.75\n",
      "Epoch 294 | Train Loss: 1.1102 | Val PPL: 460.37\n",
      "Epoch 295 | Train Loss: 1.1548 | Val PPL: 498.89\n",
      "Epoch 296 | Train Loss: 1.8108 | Val PPL: 511.32\n",
      "Epoch 297 | Train Loss: 2.3370 | Val PPL: 438.70\n",
      "Epoch 298 | Train Loss: 2.0838 | Val PPL: 444.85\n",
      "Epoch 299 | Train Loss: 2.5854 | Val PPL: 619.50\n",
      "Epoch 300 | Train Loss: 2.8810 | Val PPL: 422.85\n",
      "Â∑≤‰øùÂ≠òËÆ≠ÁªÉÊõ≤Á∫øÂõæÂÉèÂà∞ pic/training_plot.png\n",
      "Â∑≤‰øùÂ≠òÈ™åËØÅÈõÜ PPL ÊîæÂ§ßÂõæÂà∞ pic/ppl_zoomed.png\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # ÈÅøÂÖçÊüê‰∫õÁÆóÂ≠êÈùûÁ°ÆÂÆöÊÄßË°å‰∏∫Ôºà‰ºöÁ®çÊÖ¢‰∏ÄÁÇπÔºâ\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)  \n",
    "# ÂàõÂª∫ÁõÆÂΩï\n",
    "save_dir = \"checkpoints\"\n",
    "pic_dir = \"pic\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "os.makedirs(pic_dir, exist_ok=True)\n",
    "\n",
    "best_model_path = os.path.join(save_dir, \"tiny_transformer_best.pth\")\n",
    "\n",
    "best_val_ppl = float('inf')\n",
    "epochs = 300\n",
    "\n",
    "train_losses = []\n",
    "val_ppls = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = get_loss(logits, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        count += 1\n",
    "\n",
    "    avg_train_loss = total_train_loss / count\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # È™åËØÅ\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_val_loss = 0\n",
    "        count = 0\n",
    "        for x, y in valid_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = get_loss(logits, y)\n",
    "            total_val_loss += loss.item()\n",
    "            count += 1\n",
    "        avg_val_loss = total_val_loss / count\n",
    "        val_ppl = math.exp(avg_val_loss)\n",
    "        val_ppls.append(val_ppl)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val PPL: {val_ppl:.2f}\")\n",
    "\n",
    "    # ‰øùÂ≠òÊ®°Âûã\n",
    "    epoch_path = os.path.join(save_dir, f\"checkpoint_epoch_{epoch+1}.pth\")\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch + 1,\n",
    "        'val_ppl': val_ppl,\n",
    "    }, epoch_path)\n",
    "\n",
    "    if val_ppl < best_val_ppl:\n",
    "        best_val_ppl = val_ppl\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'epoch': epoch + 1,\n",
    "            'val_ppl': val_ppl,\n",
    "        }, best_model_path)\n",
    "\n",
    "    # ÊèêÂâçÂÅúÊ≠¢Êù°‰ª∂\n",
    "    if val_ppl < 30:\n",
    "        print(f\"üéâ ÊèêÂâçÂÅúÊ≠¢ÔºöÈ™åËØÅÈõÜ PPL ËææÂà∞ {val_ppl:.2f} < 30\")\n",
    "        break\n",
    "\n",
    "# ÁîªÂõæÔºàÂè™ÁîªÂ∑≤ËÆ≠ÁªÉËΩÆÊï∞Ôºâ\n",
    "epochs_ran = range(1, len(train_losses) + 1)\n",
    "\n",
    "# Âõæ 1ÔºöÂÆåÊï¥Êõ≤Á∫ø\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs_ran, train_losses, label='Train Loss')\n",
    "plt.plot(epochs_ran, val_ppls, label='Val PPL')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Training Loss & Validation Perplexity')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plot_path = os.path.join(pic_dir, 'training_plot.png')\n",
    "plt.savefig(plot_path)\n",
    "print(f\"Â∑≤‰øùÂ≠òËÆ≠ÁªÉÊõ≤Á∫øÂõæÂÉèÂà∞ {plot_path}\")\n",
    "plt.close()\n",
    "\n",
    "# Âõæ 2ÔºöÊîæÂ§ßÁâà PPL<200\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs_ran, val_ppls, label='Val PPL (zoomed)')\n",
    "plt.axhline(y=200, color='gray', linestyle='--', linewidth=1)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('PPL')\n",
    "plt.title('Validation PPL (Zoomed View, <200)')\n",
    "plt.ylim(0, 200)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "zoom_path = os.path.join(pic_dir, 'ppl_zoomed.png')\n",
    "plt.savefig(zoom_path)\n",
    "print(f\"Â∑≤‰øùÂ≠òÈ™åËØÅÈõÜ PPL ÊîæÂ§ßÂõæÂà∞ {zoom_path}\")\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abf435e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b512a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
